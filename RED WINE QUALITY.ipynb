import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,mean_absolute_error
from sklearn.metrics import accuracy_score
from math import sqrt
import matplotlib.pyplot as plt
from imblearn.combine import SMOTETomek
from sklearn.model_selection import KFold, cross_val_score
from sklearn import tree

from six import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
!pip install pydotplus
import pydotplus
     
Requirement already satisfied: pydotplus in /usr/local/lib/python3.10/dist-packages (2.0.2)
Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pydotplus) (3.1.1)

url = "/content/winequality-red.csv"
data = pd.read_csv(url)
     

data.head()

     
fixed acidity	volatile acidity	citric acid	residual sugar	chlorides	free sulfur dioxide	total sulfur dioxide	density	pH	sulphates	alcohol	quality
0	7.4	0.70	0.00	1.9	0.076	11.0	34.0	0.9978	3.51	0.56	9.4	5
1	7.8	0.88	0.00	2.6	0.098	25.0	67.0	0.9968	3.20	0.68	9.8	5
2	7.8	0.76	0.04	2.3	0.092	15.0	54.0	0.9970	3.26	0.65	9.8	5
3	11.2	0.28	0.56	1.9	0.075	17.0	60.0	0.9980	3.16	0.58	9.8	6
4	7.4	0.70	0.00	1.9	0.076	11.0	34.0	0.9978	3.51	0.56	9.4	5

data.shape
     
(1599, 12)
Feature Selection


features = ['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides',
            'free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']
     

target = ['quality']
     
Checking Null Values


data.isnull().any()

     
fixed acidity           False
volatile acidity        False
citric acid             False
residual sugar          False
chlorides               False
free sulfur dioxide     False
total sulfur dioxide    False
density                 False
pH                      False
sulphates               False
alcohol                 False
quality                 False
dtype: bool
Normalize Dataset


from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
normal_df = scaler.fit_transform(data.iloc[:,0:-1])
normal_df = pd.DataFrame(normal_df, columns = data.iloc[:,0:-1].columns)
print(normal_df.head())
     
   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \
0       0.247788          0.397260         0.00        0.068493   0.106845   
1       0.283186          0.520548         0.00        0.116438   0.143573   
2       0.283186          0.438356         0.04        0.095890   0.133556   
3       0.584071          0.109589         0.56        0.068493   0.105175   
4       0.247788          0.397260         0.00        0.068493   0.106845   

   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \
0             0.140845              0.098940  0.567548  0.606299   0.137725   
1             0.338028              0.215548  0.494126  0.362205   0.209581   
2             0.197183              0.169611  0.508811  0.409449   0.191617   
3             0.225352              0.190813  0.582232  0.330709   0.149701   
4             0.140845              0.098940  0.567548  0.606299   0.137725   

    alcohol  
0  0.153846  
1  0.215385  
2  0.215385  
3  0.215385  
4  0.153846  
Fixing Imbalanced Dataset


quality_dist = data['quality'].value_counts()
plt.bar(quality_dist.index, quality_dist)
plt.xlabel('quality')
plt.ylabel('frequency')
plt.show()
print(quality_dist)

     

5    681
6    638
7    199
4     53
8     18
3     10
Name: quality, dtype: int64

data['quality'] = data['quality'].map({
        3 : 0,
        4 : 0,
        5 : 1,
        6 : 1,
        7 : 2,
        8 : 2
})

     

fig, ax = plt.subplots(figsize=(12,8))
quality_dist = data['quality'].value_counts()
plt.bar(quality_dist.index, quality_dist)
plt.xlabel('quality')
plt.ylabel('frequency')
plt.show()
print(quality_dist)

     

1    1319
2     217
0      63
Name: quality, dtype: int64

fig, ax = plt.subplots(figsize=(12,8))
quality_dist = data['quality'].value_counts()
plt.bar(quality_dist.index, quality_dist)
plt.xlabel('quality')
plt.ylabel('frequency')
plt.show()
print(quality_dist)
     

1    1319
2     217
0      63
Name: quality, dtype: int64

sme = SMOTETomek(random_state=42)
X_resampled, y_resampled = sme.fit_resample(normal_df, data['quality'])
     

quality_dist = y_resampled.value_counts()
plt.bar(quality_dist.index, quality_dist)
plt.xlabel('quality')
plt.ylabel('frequency')
plt.show()
print(quality_dist)
     

2    1318
0    1317
1    1316
Name: quality, dtype: int64

class Percent(float):
    def __str__(self):
        return '{:.2%}'.format(self)
     
Data Visualization


def hist_plots(data):
    plt.figure(figsize=(10, 6))
    plt.hist(data)
    plt.title("Histogram Plot")
    plt.show()
hist_plots(data['fixed acidity'])


     


hist_plots(data['pH'])
     


hist_plots(data['quality'])
     


hist_plots(data['alcohol'])
     


hist_plots(data['residual sugar'])

     


import seaborn as sns
     

sns.kdeplot(data['volatile acidity'])
     
<Axes: xlabel='volatile acidity', ylabel='Density'>


sns.kdeplot(data['chlorides'])
     
<Axes: xlabel='chlorides', ylabel='Density'>


sns.kdeplot(data['density'])
     
<Axes: xlabel='density', ylabel='Density'>


plt.figure(figsize = (12,6))
sns.pairplot(data)
plt.show()

     
<Figure size 1200x600 with 0 Axes>


plt.figure(figsize = (12,6))
sns.heatmap(data.corr())
plt.show()

     


plt.figure(figsize = (12,6))
sns.scatterplot(x='pH', y = 'density', data = data)
plt.show()
     


plt.figure(figsize = (12,6))
sns.scatterplot(x='fixed acidity', y = 'pH', data = data)
plt.show()

     


plt.figure(figsize = (12,6))
sns.scatterplot(x='volatile acidity', y = 'pH', data = data)
plt.show()

     


data.hist(figsize=(11, 11))
plt.show()

     


data['quality'].unique()
     
array([1, 2, 0])

data['quality'].value_counts()

     
1    1319
2     217
0      63
Name: quality, dtype: int64
Linear Regression Model


from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
     

X=data.drop(['quality'], axis=1)
y=data['quality']
     

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 26)
model = LinearRegression()
model.fit(X_train, y_train)

     
LinearRegression()
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.


model.coef_
     
array([ 1.84329460e-02, -4.62000725e-01,  1.76420071e-02,  6.35145828e-03,
       -1.00133332e+00, -5.38649286e-04, -1.93748618e-04, -1.19277431e+01,
       -1.38148886e-01,  3.84439947e-01,  1.07000096e-01])

model.intercept_
     
12.250887105898576

cdf = pd.DataFrame(model.coef_, X.columns, columns = ['coef'])
cdf
     
coef
fixed acidity	0.018433
volatile acidity	-0.462001
citric acid	0.017642
residual sugar	0.006351
chlorides	-1.001333
free sulfur dioxide	-0.000539
total sulfur dioxide	-0.000194
density	-11.927743
pH	-0.138149
sulphates	0.384440
alcohol	0.107000

predictions = model.predict(X_test)
predictions

     
array([1.14201498, 1.24654766, 0.92105229, 0.70582456, 0.87045629,
       1.27630812, 1.33979559, 1.07418721, 1.25295678, 0.91234221,
       0.9132498 , 1.19453465, 1.28541619, 0.84315456, 1.43803379,
       1.47223943, 1.43689063, 1.01122107, 0.7914708 , 0.76055307,
       1.10224057, 1.23355159, 0.94491132, 0.72685617, 1.08012254,
       0.76234533, 1.0107731 , 1.23939341, 0.87693956, 0.98779481,
       1.11371114, 1.24432745, 1.24088435, 1.31421807, 0.9085376 ,
       1.10999656, 1.47761511, 0.84476187, 0.83289533, 0.86459045,
       0.84558469, 1.22300995, 0.93110299, 1.25280258, 0.84491972,
       1.12870278, 1.333494  , 1.63711649, 0.86757754, 0.98341625,
       1.00508086, 0.96003544, 0.99456742, 0.90375001, 0.93102036,
       0.90061281, 1.46494776, 1.25387649, 1.01150359, 1.40669321,
       0.73757382, 1.3231717 , 1.23492293, 1.12131202, 0.90526786,
       1.02988872, 1.30665783, 1.0572288 , 1.01424381, 1.1409408 ,
       1.10020935, 0.95297287, 1.38556291, 0.81528396, 1.35998769,
       0.95937117, 0.97668246, 0.98567626, 1.07718435, 0.95297287,
       0.8357222 , 0.94855952, 0.90526786, 0.95372392, 0.9427113 ,
       0.80330718, 0.84460764, 1.05099351, 0.93589296, 1.05794823,
       1.06182487, 1.14838493, 0.99040307, 0.89203791, 1.47791614,
       0.96104246, 1.16238867, 0.89951435, 0.84050359, 0.81816214,
       0.93561962, 1.21801462, 1.25508411, 0.85099575, 1.23935194,
       1.01049057, 1.29418008, 1.50201426, 1.02479129, 1.05349381,
       1.06182487, 0.89269149, 0.91301347, 1.34828754, 1.11394789,
       1.07955151, 1.18773974, 0.87939527, 0.87681883, 1.1640934 ,
       0.81911165, 0.8357222 , 0.86485779, 1.05366269, 0.76377874,
       1.27427829, 0.8746812 , 0.94932626, 1.28104534, 0.99514601,
       0.83751345, 1.30011873, 1.15717413, 1.20899742, 0.96325551,
       1.06072818, 1.23003788, 1.03379209, 1.3220126 , 0.82184719,
       1.10772173, 1.11549686, 1.43785876, 1.08932761, 1.23717373,
       1.32700739, 0.50404894, 1.12870278, 0.96376209, 0.93465548,
       1.08713169, 1.14201498, 1.02913658, 1.0836138 , 1.09453671,
       1.01531246, 1.22767311, 1.13401728, 1.47930004, 1.18838746,
       0.89317659, 0.96443991, 1.40792318, 1.22649668, 0.95126763,
       0.90843668, 0.91842085, 1.16475489, 1.07366055, 0.91226577,
       1.33181079, 0.9513969 , 1.31400169, 0.99853104, 1.07534879,
       1.09740647, 0.93531007, 1.00023626, 0.95191054, 0.86761951,
       0.97302744, 0.95060751, 1.17300726, 1.12073641, 0.99156363,
       1.11741497, 0.7690717 , 0.95023429, 0.81393924, 1.1459247 ,
       1.09810338, 1.56536997, 1.18644182, 0.78121291, 1.58791976,
       0.93259991, 1.40938473, 1.19024115, 0.7580353 , 1.00135243,
       1.19453465, 0.94994074, 1.01150359, 0.86149941, 0.93406064,
       0.98149198, 0.92593957, 1.18577385, 1.00548082, 0.93259991,
       1.05724199, 0.97965591, 1.3155891 , 0.99536704, 1.31526862,
       1.36782421, 1.07036597, 1.15690554, 1.14113706, 1.46236492,
       1.29282068, 1.48195789, 0.84064959, 1.09184726, 0.84982772,
       1.13897841, 1.14481668, 0.96669368, 1.37158145, 0.9910311 ,
       1.39529078, 0.98049934, 1.16930903, 1.18838746, 0.82688687,
       1.11433467, 1.25847457, 0.91357391, 1.04232271, 1.3702147 ,
       1.30184985, 1.00818801, 1.2847296 , 1.16306295, 0.93259991,
       0.94491132, 0.99588448, 0.94566317, 1.12487703, 1.13435451,
       1.22809466, 1.12694779, 1.04430683, 1.09984254, 0.99746728,
       1.13350751, 0.82512468, 1.17300397, 1.33875439, 0.85895081,
       0.972935  , 1.00835323, 0.96364561, 0.87432928, 1.20500171,
       1.0932634 , 0.94968411, 0.92474737, 0.96616236, 1.13036085,
       1.03071453, 0.74121618, 1.27833285, 1.30015195, 1.30509521,
       1.50191635, 1.29360173, 1.12663781, 0.79515016, 1.15368557,
       1.44867275, 0.76804706, 1.27938708, 1.23834345, 1.14291267,
       0.83486983, 0.9091131 , 0.95022084, 1.11256781, 0.82202683,
       1.10948135, 1.07789022, 0.98422157, 1.35209067, 1.45900101,
       1.12434073, 1.48597318, 0.83896164, 1.06212349, 1.13215725,
       1.11371114, 1.00624146, 0.97306063, 1.1066567 , 0.94195266,
       1.27029661, 1.00123181, 1.37755614, 0.80669229, 1.59004536,
       0.90938861, 1.25418526, 0.95293968, 1.27601718, 1.08229565,
       0.953517  , 1.19913449, 0.95126763, 0.96364561, 1.1530372 ,
       0.94593386, 0.85764719, 1.31473192, 0.97623246, 0.91503126,
       0.8745798 , 0.98150912, 0.93465548, 1.21755328, 1.32294851,
       1.41139163, 1.19864048, 0.9508052 , 1.25021242, 0.92440712,
       1.15063307, 1.06916942, 1.04723454, 1.20422633, 1.33808848,
       1.51439752, 1.10777468, 1.19304381, 1.08467742, 1.23998682,
       0.98142668, 1.2258952 , 1.03963928, 0.7944137 , 1.35768951,
       1.54771641, 0.82401509, 1.33061007, 1.06615031, 0.98212202,
       1.20859706, 0.98957906, 1.53314259, 0.90938861, 1.14675546,
       1.05226327, 1.27206777, 1.2122257 , 0.9501227 , 0.94340269,
       1.54414974, 1.01063484, 1.20859706, 0.90045457, 1.18462482,
       0.95590751, 0.98074621, 1.02598304, 1.0005035 , 1.02788738,
       1.37041621, 1.06859805, 0.87231862, 1.11995314, 1.16261773,
       1.08434642, 0.98837204, 1.0069333 , 0.99122538, 1.16930998,
       0.9508945 , 0.85599306, 1.4311946 , 0.9558349 , 1.13460093,
       1.00023626, 0.99062281, 0.82939052, 1.11759927, 0.86490996,
       1.01137906, 1.29854799, 1.13157133, 1.34438867, 1.17761306,
       1.10721523, 1.24285977, 1.24987776, 1.08859969, 0.90240674,
       0.78598384, 1.36610634, 0.71450045, 1.15433053, 1.30364725,
       1.3344203 , 0.87522121, 1.25843817, 0.86271399, 1.19183745,
       1.06182487, 0.95461616, 0.96496617, 0.92338868, 0.74512265,
       1.38154617, 0.90320874, 0.75841803, 0.89778108, 0.91717874,
       0.94913462, 1.05185396, 1.00808122, 1.39642268, 1.11256781,
       1.52377018, 1.27648707, 0.82004131, 1.07779407, 0.80826104,
       1.44152567, 1.17713834, 0.90934026, 1.18224586, 1.3220126 ,
       0.76996764, 1.11759927, 1.20457023, 1.23215144, 0.71007711,
       1.17848635, 1.07111046, 0.95126763, 1.22734415, 1.1408618 ,
       1.02913658, 1.03561497, 1.44017101, 0.93721847, 0.98181369,
       1.00299488, 0.9051726 , 1.01652317, 0.95621972, 1.02171358,
       1.38765382, 0.93743327, 1.26324529, 0.85632523, 1.34979655,
       1.36253417, 0.91234221, 1.33706765, 1.38075378, 1.43221471,
       1.16670922, 1.10224057, 1.01084574, 0.83863326, 0.82184719,
       1.03660832, 1.16475489, 1.44152567, 1.50126932, 0.86349311])
Decision Tree


from sklearn.tree import DecisionTreeClassifier
dt= DecisionTreeClassifier()
dt.fit(X_train,y_train)
predictiondt= dt.predict(X_test)
predictiondt

     
array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 2, 1, 1, 1, 0, 2,
       1, 1, 1, 1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1,
       1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
       1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0,
       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 2,
       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1,
       2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 2, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 2,
       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1,
       1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,
       1, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 2, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 0,
       1, 1, 2, 2, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1])

accuracy_score(y_test, predictiondt)
     
0.8041666666666667
Random Forest


from sklearn.ensemble import RandomForestClassifier
rf= RandomForestClassifier()
rf.fit(X_train,y_train)
predictionrf= rf.predict(X_test)
predictionrf
     
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1,
       1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,
       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1])

accuracy_score(y_test, predictionrf)

     
0.8875
